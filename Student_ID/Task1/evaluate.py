import os
import glob
import re
import numpy as np
from sklearn.metrics import f1_score
from model import Config, build_model
from dataset import get_test_paths, data_loader

def get_all_checkpoints(dir_path):
    """æ‰¾åˆ°ç›®å½•ä¸‹æ‰€æœ‰çš„æ¨¡å‹æ–‡ä»¶ (.pth)"""
    # åŒ¹é… model_epoch_X.pth, model_final.pth, best_model.pth
    patterns = [
        os.path.join(dir_path, 'model_epoch_*.pth'),
        os.path.join(dir_path, 'model_final.pth'),
        os.path.join(dir_path, 'best_model.pth'),
        os.path.join(dir_path, 'best_loss_model.pth')
    ]
    files = []
    for p in patterns:
        files.extend(glob.glob(p))
    
    # å»é‡å¹¶æ’åº
    files = sorted(list(set(files)))
    return files

def evaluate_single_model(model, test_data, batch_size):
    """å¯¹å•ä¸ªæ¨¡å‹è¿›è¡Œæ¨ç†å¹¶è¿”å›é¢„æµ‹ç»“æœ"""
    preds, targets = [], []
    for X, Y in data_loader(test_data, batch_size, training=False):
        out = model.forward(X, training=False)
        preds.extend(out.cpu().numpy().flatten())
        targets.extend(Y.cpu().numpy().flatten())
    return np.array(preds), np.array(targets)

def search_best_threshold(preds, targets):
    """ä¸ºå•ä¸ªæ¨¡å‹æœç´¢æœ€ä½³é˜ˆå€¼"""
    best_f1 = 0
    best_thresh = 0.5
    # ç²—ç•¥æœç´¢ + ç²¾ç»†æœç´¢
    for t in [0.5, 0.6, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95]:
        f1 = f1_score(targets, (preds > t).astype(int))
        if f1 > best_f1:
            best_f1 = f1
            best_thresh = t
    return best_f1, best_thresh

def main():
    print(f"[Eval] Device: {Config.device}")
    
    # 1. å‡†å¤‡æ•°æ®
    test_data = get_test_paths(Config.data_dir)
    if not test_data:
        print("[Error] No test data found.")
        return
    print(f"[Eval] Loaded {len(test_data)} test samples.")

    # 2. æ‰¾åˆ°æ‰€æœ‰æ¨¡å‹
    model_files = get_all_checkpoints(Config.current_dir)
    if not model_files:
        print("[Error] No .pth models found. Train first!")
        return
        
    print(f"[Eval] Found {len(model_files)} models. Starting competition...\n")
    print(f"{'Model Name':<25} | {'Best F1':<10} | {'Threshold':<10}")
    print("-" * 50)

    # 3. é€ä¸ªè¯„ä¼°
    global_best_f1 = 0
    global_best_model = ""
    global_best_thresh = 0.5
    
    # åˆå§‹åŒ–æ¨¡å‹ç»“æ„ (åªå»ºä¸€æ¬¡ï¼Œåé¢åªè¿™å°±load_state_dict)
    model = build_model()
    
    for model_path in model_files:
        model_name = os.path.basename(model_path)
        try:
            model.load(model_path)
            
            # æ¨ç†
            preds, targets = evaluate_single_model(model, test_data, Config.batch_size)
            
            # æ‰¾è¯¥æ¨¡å‹çš„æœ€ä½³è¡¨ç°
            f1, thresh = search_best_threshold(preds, targets)
            
            print(f"{model_name:<25} | {f1:.4f}     | {thresh:<10}")
            
            # æ›´æ–°å…¨å±€æœ€ä½³
            if f1 > global_best_f1:
                global_best_f1 = f1
                global_best_model = model_name
                global_best_thresh = thresh
                
        except Exception as e:
            print(f"{model_name:<25} | Error: {str(e)}")

    # 4. æ€»ç»“
    print("-" * 50)
    print(f"\nğŸ† WINNER: {global_best_model}")
    print(f"ğŸ¥‡ Max F1 Score: {global_best_f1:.4f}")
    print(f"ğŸ”‘ Optimal Threshold: {global_best_thresh}")
    
    print("\n>>> æ“ä½œæ­¥éª¤ (Action Items):")
    print(f"1. è¯·å°†æ–‡ä»¶ '{global_best_model}' é‡å‘½åä¸º 'best_model.pth'")
    print(f"   å‘½ä»¤: mv {global_best_model} best_model.pth")
    print(f"2. ä¿®æ”¹ model.py ä¸­çš„ Config.threshold = {global_best_thresh}")

if __name__ == "__main__":
    main()
